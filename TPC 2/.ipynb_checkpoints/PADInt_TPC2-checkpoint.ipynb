{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning, Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2. Markov decision problems\n",
    "\n",
    "\n",
    "Consider an agent moving in the grid-world environment of Fig. 1. The agent must reach\n",
    "the goal cell, marked with “G”.  \n",
    "\n",
    "At each step, the agent may move in any of the four directions—up, down, left and right.\n",
    "Movement across a grey cell division succeeds with a 0.8 probability and fails with a 0.2 probability.\n",
    "Movements across colored cell divisions (blue or red) succeed with a 0.8 probability only\n",
    "if the agent has the corresponding colored key. Otherwise, they fail with probability 1. When\n",
    "the movement fails, the agent remains in the same cell.\n",
    "\n",
    "To get a colored key, the agent simply needs to stand in the corresponding cell. In other\n",
    "words, as soon as the agent stands on the cell of a colored key, you may consider that it holds\n",
    "that key thereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**\n",
    "\n",
    "We assigned each grid position of the environment, a letter, from A to G, such that, the newly drawn board is as presented below:\n",
    "```\n",
    "| A | B | E |  \n",
    "    | C | F | G |  \n",
    "    | D |  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. \n",
    "### Identify the state space, X , and the action space, A, for the MDP. Assume that the agent never has the blue key without the red key and never reaches the goal without both keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: ()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = ('B0', 'C0', 'E0', 'F0', 'B1', 'C1', 'D1', 'E1', 'F1', 'A2', 'B2', 'C2', 'D2', 'E2', 'E2', 'F2', 'G2')\n",
    "print(\"States: {}\".format(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><em>TODO</em></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.b. \n",
    "### Write down the transition probability matrix for the action “right” and a (possible) cost function for the MDP. \n",
    "### Make sure that the cost function is as simple as possible and verifies c(x, a) ∈ [0, 1] for all states x ∈ X and actions a ∈ A. Note, in particular, that the cost should depend only on the agent standing in the goal cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><em>TODO</em></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c.\n",
    "### Compute the cost-to-go function associated with the policy in which the agent always goes right, using a discount γ = 0.9. You can use any software of your liking for the harder computations, but should indicate all other computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><em>TODO</em></span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
