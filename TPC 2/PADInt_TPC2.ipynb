{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning, Learning and Decision Making\n",
    "\n",
    "Group 27\n",
    "\n",
    "78375 - João Pirralha\n",
    "\n",
    "84758 - Rafael Ribeiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2. Markov decision problems\n",
    "\n",
    "\n",
    "Consider an agent moving in the grid-world environment of Fig. 1. The agent must reach\n",
    "the goal cell, marked with “G”.  \n",
    "\n",
    "At each step, the agent may move in any of the four directions—up, down, left and right.\n",
    "Movement across a grey cell division succeeds with a 0.8 probability and fails with a 0.2 probability.\n",
    "Movements across colored cell divisions (blue or red) succeed with a 0.8 probability only\n",
    "if the agent has the corresponding colored key. Otherwise, they fail with probability 1. When\n",
    "the movement fails, the agent remains in the same cell.\n",
    "\n",
    "To get a colored key, the agent simply needs to stand in the corresponding cell. In other\n",
    "words, as soon as the agent stands on the cell of a colored key, you may consider that it holds\n",
    "that key thereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**\n",
    "\n",
    "We assigned each grid position of the environment, a letter, from A to G, such that, the newly drawn board is as presented below:\n",
    "```\n",
    "| A | B | E |  \n",
    "    | C | F | G |  \n",
    "    | D |  \n",
    "```\n",
    "\n",
    "B0 means in cell B with no keys; B1 in cell B with the red key; and B2 in cell B with both keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. \n",
    "### Identify the state space, X , and the action space, A, for the MDP. Assume that the agent never has the blue key without the red key and never reaches the goal without both keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: ('B0', 'C0', 'E0', 'F0', 'B1', 'C1', 'D1', 'E1', 'F1', 'A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2')\n",
      "Actions: ('U', 'D', 'L', 'R')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#States\n",
    "X = ('B0', 'C0', 'E0', 'F0', 'B1', 'C1', 'D1', 'E1', 'F1', 'A2', 'B2', 'C2', 'D2', 'E2', 'F2', 'G2')\n",
    "print(\"States: {}\".format(X))\n",
    "\n",
    "#Actions\n",
    "A = ('U', 'D', 'L', 'R')\n",
    "print(\"Actions: {}\".format(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.b. \n",
    "### Write down the transition probability matrix for the action “right” and a (possible) cost function for the MDP. \n",
    "### Make sure that the cost function is as simple as possible and verifies c(x, a) ∈ [0, 1] for all states x ∈ X and actions a ∈ A. Note, in particular, that the cost should depend only on the agent standing in the goal cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Probability Matrix for Right action: \n",
      "[[0.2 0.  0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.2 0.  0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.2 0.  0.  0.8 0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.2 0.  0.  0.8 0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.8 0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.8 0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.  0.  0.8 0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.8]\n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. ]]\n",
      "\n",
      "Cost Function: \n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Outcomes\n",
    "#Action R(ight)\n",
    "PR = np.zeros((len(X), (len(X))))\n",
    "PR[0, 0] = 0.2\n",
    "PR[0, 2] = 0.8\n",
    "PR[1, 1] = 0.2\n",
    "PR[1, 3] = 0.8\n",
    "PR[2, 2] = 1\n",
    "PR[3, 3] = 1\n",
    "PR[4, 4] = 0.2\n",
    "PR[4, 7] = 0.8\n",
    "PR[5, 5] = 0.2\n",
    "PR[5, 8] = 0.8\n",
    "PR[6, 6] = 1\n",
    "PR[7, 7] = 1\n",
    "PR[8, 8] = 1\n",
    "PR[9, 9] = 0.2\n",
    "PR[9, 10] = 0.8\n",
    "PR[10, 10] = 0.2\n",
    "PR[10, 13] = 0.8\n",
    "PR[11, 11] = 0.2\n",
    "PR[11, 14] = 0.8\n",
    "PR[12, 12] = 1\n",
    "PR[13, 13] = 1\n",
    "PR[14, 14] = 0.2\n",
    "PR[14, 15] = 0.8\n",
    "PR[15, 15] = 1\n",
    "print(\"Transition Probability Matrix for Right action: \\n{}\\n\".format(PR))\n",
    "\n",
    "#Cost\n",
    "C = np.zeros((len(X), len(A)))\n",
    "C[:15] = 1\n",
    "C[14, 3] = 0\n",
    "C[15, 0:2] = 0\n",
    "C[15, 3] = 0\n",
    "C[15, 2] = 1\n",
    "C = np.round(C / np.max(C), 3) #Making sure all the costs are between 0 and 1, in case a weight is changed acidentaly\n",
    "print(\"Cost Function: \\n{}\\n\".format(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.c.\n",
    "### Compute the cost-to-go function associated with the policy in which the agent always goes right, using a discount γ = 0.9. You can use any software of your liking for the harder computations, but should indicate all other computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cost function defined in 1.b.:\n",
    "\n",
    "* $J^\\rightarrow_{G2} = 0\\ (goal\\ state)$\n",
    "* $J^\\rightarrow_{F2} = 0.8\\times0 + 0.2\\times(0+γ\\times(0.8\\times0) + γ\\times(0.2\\times(0+γ\\times(0.8\\times0) + γ\\times(0.2\\times(0+\\ldots))))) = 0$\n",
    "* $J^\\rightarrow_{C2} = 0.8\\times(1+γ\\times{J^\\rightarrow_{F2}})\n",
    "  + 0.2\\times(1 + γ\\times(0.8\\times(1+γ\\times{J^\\rightarrow_{F2}}))\n",
    "  + γ\\times(0.2\\times(1+\\ldots))$\n",
    "  $$ = 0.8\\times(1+γ\\times{J^\\rightarrow_{F2}})\n",
    "  + 0.2 + (0.2\\timesγ)\\times(0.8\\times(1+γ\\times{J^\\rightarrow_{F2}}))\n",
    "  + (0.2\\timesγ)\\times0.2 + (0.2\\timesγ)^2\\times(0.8\\times(1+γ\\times{J^\\rightarrow_{F2}})) + \\ldots $$\n",
    "  $$ = (0.8\\times(1+γ\\times{J^\\rightarrow_{F2}}) + 0.2\\times1) \\times(1 + (0.2\\timesγ) + (0.2\\timesγ)^2 + \\ldots)$$\n",
    "  $$ = (0.8\\times(1+γ\\times{J^\\rightarrow_{F2}}) + 0.2\\times1) \\times \\sum_{n=0}^{\\infty} {(0.2\\timesγ)^n} $$\n",
    "  $$ = (0.8\\times(1+γ\\times{J^\\rightarrow_{F2}}) + 0.2\\times1) \\times \\frac1{1 - 0.2\\timesγ}$$\n",
    "  $$ = (0.8\\times(1+0.9\\times 0) + 0.2\\times1) \\times \\frac1{1 - 0.2\\times0.9}$$\n",
    "  $$ = (0.8 + 0.2) \\times \\frac1{1 - 0.2\\times0.9} $$\n",
    "  $$ \\approx 1.2195 $$\n",
    "  \n",
    "\n",
    "The remaining states will never reach the goal state with the policy in which the agent always goes right, so the agent will be penalized forever:\n",
    "* $J^\\rightarrow_{remaining} = 1 + 1\\timesγ + 1\\timesγ^2 + \\ldots = \\sum_{n=0}^{\\infty} {γ^n} = \\frac1{1 - γ} = 10$\n",
    "\n",
    "Thus:\n",
    "$$J^\\rightarrow = \\begin{bmatrix} 10\\\\ 10\\\\ 10\\\\ 10\\\\ 10\\\\ 10\\\\ 10\\\\ 10\\\\ 10\\\\ 10\\\\ 10\\\\ 1.2195\\\\ 10\\\\ 10\\\\ 0\\\\ 0 \\end{bmatrix}$$\n",
    "\n",
    "Next we experimentally validade our answer for $J^\\rightarrow_{F2}$ and $J^\\rightarrow_{C2}$, which are the most difficult:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Simulation) J→F2:\t 0.0\n",
      "(Simulation) J→C2:\t 1.2207\n"
     ]
    }
   ],
   "source": [
    "# Simulation for validation\n",
    "ITERATIONS = 10**5\n",
    "GAMMA = 0.9\n",
    "ACTION = 3 # Always move right\n",
    "simStates = np.arange(16)\n",
    "\n",
    "def simulate(initialState):\n",
    "    cost = 0\n",
    "    for i in range(ITERATIONS):\n",
    "        state = initialState\n",
    "        j = 0\n",
    "        while state != 15:\n",
    "            cost += C[state, ACTION] * GAMMA ** j\n",
    "            state = np.random.choice(simStates, p=PR[state])\n",
    "            j += 1\n",
    "    return cost / ITERATIONS\n",
    "\n",
    "print(\"(Simulation) J→F2:\\t {}\".format(round(simulate(14), 4)))\n",
    "print(\"(Simulation) J→C2:\\t {}\".format(round(simulate(11), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, can also compute J systematically using the second equation in slide 39 of lec8.pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.       ]\n",
      " [10.       ]\n",
      " [10.       ]\n",
      " [10.       ]\n",
      " [10.       ]\n",
      " [10.       ]\n",
      " [10.       ]\n",
      " [10.       ]\n",
      " [10.       ]\n",
      " [10.       ]\n",
      " [10.       ]\n",
      " [ 1.2195122]\n",
      " [10.       ]\n",
      " [10.       ]\n",
      " [ 0.       ]\n",
      " [ 0.       ]]\n"
     ]
    }
   ],
   "source": [
    "J = np.linalg.inv(np.eye(len(X)) - 0.9 * PR) @ C[:,3]\n",
    "print(J.reshape((len(X), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
