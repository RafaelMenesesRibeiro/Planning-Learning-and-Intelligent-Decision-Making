{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laboratory 5: Reinforcement learning\n",
    "\n",
    "In the end of the lab, you should submit all code/answers written in the tasks marked as \"Activity n. XXX\", together with the corresponding outputs and any replies to specific questions posed to the e-mail <adi.tecnico@gmail.com>. Make sure that the subject is of the form [&lt;group n.&gt;] LAB &lt;lab n.&gt;."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The key world domain\n",
    "\n",
    "Consider once again the gridworld domain from Lab 2 and which you modeled using a Markov decision process.\n",
    "\n",
    "<img src=\"maze.png\" width=\"200px\">\n",
    "\n",
    "Recall that:\n",
    "\n",
    "* At each step, the agent may move in any of the four directions -- up, down, left and right.\n",
    "\n",
    "* Movement across a _grey_ cell division succeeds with a $0.8$ probability and fails with a $0.2$ probability. \n",
    "\n",
    "* Movements across colored cell divisions (blue or red) succeed with a $0.8$ probability _but only if the agent has the corresponding colored key_. Otherwise, they fail with probability $1$. \n",
    "\n",
    "* When the movement fails, the agent remains in the same cell. \n",
    "\n",
    "* To get a colored key, the agent simply needs to stand in the corresponding cell. \n",
    "\n",
    "* The goal of the agent is to reach the cell marked with **\"G\"**. \n",
    "\n",
    "Throughout the lab, use $\\gamma=0.99$. As seen in Lab 2, this problem can be modeled as a Markov decision problem $(\\mathcal{X},\\mathcal{A},\\{\\mathbf{P_a}\\},c,\\gamma\\}$ as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# States\n",
    "X = ['1BR', '2', '2R', '2BR', '3', '3R', '3BR', '4', '4R', '4BR', '5', '5R', '5BR', '6BR', '7R', '7BR']\n",
    "\n",
    "nX = len(X)\n",
    "\n",
    "# Actions\n",
    "A = ['U', 'D', 'L', 'R']\n",
    "\n",
    "nA = len(A)\n",
    "\n",
    "# Transition probabilities for the hare\n",
    "U = np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2]])\n",
    "\n",
    "D = np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "L = np.array([[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.8, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.2, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "R = np.array([[0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.8, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]])\n",
    "\n",
    "P = [U, D, L, R]\n",
    "\n",
    "# Cost function\n",
    "             \n",
    "c = np.array([[1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [0.0, 0.0, 0.0, 0.0],\n",
    "              [1.0, 1.0, 1.0, 1.0],\n",
    "              [1.0, 1.0, 1.0, 1.0]])\n",
    "\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 1.        \n",
    "\n",
    "Compute the optimal $Q$-function for the MDP defined above using value iteration. As your stopping condition, use an error between iterations smaller than `1e-8`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.84607096  5.84607096  5.84607096  4.89502117]\n",
      " [11.57144785 10.67823015 11.57144785 12.45352816]\n",
      " [ 7.0200601   7.9475408   6.08086879  7.9475408 ]\n",
      " [ 4.65725873  3.69420073  5.60830851  3.69420073]\n",
      " [12.67404824 11.79196792 11.79196792 12.67404824]\n",
      " [ 8.17941097  9.09532707  7.25193028  8.17941097]\n",
      " [ 3.45343623  2.47821842  4.41649423  3.45343623]\n",
      " [11.34814342  9.55043002 10.45492572 11.34814342]\n",
      " [ 7.25193028  9.09532707  8.17941097  9.09532707]\n",
      " [ 4.41649423  4.41649423  3.45343623  2.47821842]\n",
      " [12.45352816 11.57144785 10.67823015 11.57144785]\n",
      " [ 8.40839     9.3243061   8.40839     9.3243061 ]\n",
      " [ 3.20963178  2.23441397  3.20963178  1.24688279]\n",
      " [ 0.          0.          0.98753117  0.        ]\n",
      " [ 8.40839     9.3243061   9.3243061   9.3243061 ]\n",
      " [ 3.69420073  4.65725873  4.65725873  4.65725873]]\n"
     ]
    }
   ],
   "source": [
    "# Reused from activities 4 and 5 in lab 2\n",
    "\n",
    "MIN_ERR = 1e-8\n",
    "\n",
    "J = np.zeros((nX, 1))\n",
    "err = 1\n",
    "\n",
    "# Based on slide 62 of lec8.pdf:\n",
    "while err > MIN_ERR:\n",
    "    Qs = [c[:, [i]] + gamma * P[i] @ J for i in range(nA)]\n",
    "    Jnew = np.min(Qs, axis=0)\n",
    "    err = np.linalg.norm(Jnew - J)\n",
    "    J = Jnew\n",
    "\n",
    "Q = np.hstack(Qs)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 2.        \n",
    "\n",
    "Write down a Python function that, given a Q-function $Q$ and a state $x$, selects a random action using the $\\epsilon$-greedy policy obtained from $Q$ for state $x$. Your function should receive an optional parameter, corresponding to $\\epsilon$, with default value of 0.1. \n",
    "\n",
    "**Note:** In the case of two actions with the same value, your $\\epsilon$-greedy policy should randomize between the two.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy(Q, x, epsilon=0.1):\n",
    "    if np.random.choice([0, 1], p=[epsilon, 1-epsilon]):\n",
    "        return np.random.choice(np.where(Q[x] == np.min(Q[x]))[0])\n",
    "    else:\n",
    "        return np.random.choice(nA)\n",
    "\n",
    "for i in range(10):\n",
    "    print(epsilon_greedy(Q, 3, 0))\n",
    "print(\"\")\n",
    "for i in range(10):\n",
    "    print(epsilon_greedy(Q, 3, 0.5))\n",
    "print(\"\")\n",
    "for i in range(10):\n",
    "    print(epsilon_greedy(Q, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Model-based learning\n",
    "\n",
    "You will now run the model-based learning algorithm discussed in class, and evaluate its learning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 3.        \n",
    "\n",
    "Run the model-based reinforcement learning algorithm discussed in class to compute $Q^*$ for $5,000$ iterations. Initialize each transition probability matrix as the identity and the cost function as all-zeros. Use an $\\epsilon$-greedy policy with $\\epsilon=0.1$ (use the function from Activity 2). Note that, at each step,\n",
    "\n",
    "* You will need to select an action according to the $\\epsilon$-greedy policy;\n",
    "* The state and action, you will then compute the cost and generate the next state; \n",
    "* With this transition information (state, action, cost, next-state), you can now perform an update. \n",
    "* When updating the components $(x,a)$ of the model, use the step-size\n",
    "\n",
    "$$\\alpha_t=\\frac{1}{N_t(x,a)+1},$$\n",
    "\n",
    "where $N_t(x,a)$ is the number of visits to the pair $(x,a)$ up to time step $t$.\n",
    "\n",
    "In order to ensure that your algorithm visits every state and action a sufficient number of times, after the boat reaches the goal cell, make one further step, the corresponding update, and then reset the position of the vehicle to a random state in the environment.\n",
    "\n",
    "Plot the norm $\\|Q^*-Q^{(k)}\\|$ every iteration of your method, where $Q^*$ is the optimal $Q$-function computed in Activity 1.\n",
    "\n",
    "**Note:** The simulation may take a bit. Don't despair.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.89550203  5.89552429  5.89246339  4.92046817]\n",
      " [11.69829117 10.7574806  11.64288907 12.51732316]\n",
      " [ 7.08572977  8.17793793  6.10443495  8.13959189]\n",
      " [ 4.71708226  3.73956344  5.57327136  3.90815692]\n",
      " [12.72905708 12.24510154 11.83711539 12.76528674]\n",
      " [ 7.77981713  7.68526664  7.33243308  7.72553056]\n",
      " [ 3.940399    2.72784094  3.940399    3.940399  ]\n",
      " [11.59589669  9.56029765 10.50041873 11.418183  ]\n",
      " [ 7.29080374  9.25174539  8.30052315  9.13805993]\n",
      " [ 4.46561819  4.48944363  3.48292641  2.49763305]\n",
      " [12.53685943 11.45382305 10.73598871 11.48595761]\n",
      " [ 8.575848    8.64827525  8.66240275  8.64827525]\n",
      " [ 3.7606176   2.2472949   3.18400845  1.2529353 ]\n",
      " [ 0.          0.          1.25158192  0.        ]\n",
      " [ 8.43787419  9.36615662  9.36908783  9.36499786]\n",
      " [ 3.69337334  4.90099501  4.90099501  4.90099501]]\n"
     ]
    }
   ],
   "source": [
    "N_ITERATIONS = 5000\n",
    "GOAL = 13\n",
    "\n",
    "U3 = np.eye(nX)\n",
    "D3 = np.eye(nX)\n",
    "L3 = np.eye(nX)\n",
    "R3 = np.eye(nX)\n",
    "P3 = [U3, D3, L3, R3]\n",
    "c3 = np.zeros((nX, nA))\n",
    "Q3 = np.zeros((nX, nA))\n",
    "N3 = np.zeros((nX, nA))\n",
    "\n",
    "def P_t1(y, x_t, a_t, x_t1, alpha):\n",
    "    P3[a_t][x_t, y] = P3[a_t][x_t, y] + alpha * (int(x_t1 == y) - P3[a_t][x_t, y])\n",
    "    return P3[a_t][x_t, y]\n",
    "\n",
    "def c_t1(x_t, a_t, c_t, alpha):\n",
    "    c3[x_t, a_t] = c3[x_t, a_t] + alpha * (c_t - c3[x_t, a_t])\n",
    "    return c3[x_t, a_t]\n",
    "\n",
    "def step_size(x_t, a_t):\n",
    "    return 1/(N3[x_t, a_t] + 1)\n",
    "\n",
    "def Q_t1(x_t, a_t, c_t, x_t1):\n",
    "    alpha = step_size(x_t, a_t)\n",
    "    result = c_t1(x_t, a_t, c_t, alpha)\n",
    "    for y in range(nX):\n",
    "        result += gamma * P_t1(y, x_t, a_t, x_t1, alpha) * np.min(Q3[y])\n",
    "    return result\n",
    "\n",
    "def reset():\n",
    "    return np.random.choice((1, 4, 7, 10)) # The 4 states without a key\n",
    "\n",
    "def simulate():\n",
    "    x_t = reset()\n",
    "    for i in range(N_ITERATIONS):\n",
    "        a_t = epsilon_greedy(Q3, x_t)\n",
    "        c_t = c[x_t, a_t]\n",
    "        x_t1 = np.random.choice(nX, p=P[a_t][x_t])\n",
    "        \n",
    "        Q3[x_t, a_t] = Q_t1(x_t, a_t, c_t, x_t1)\n",
    "        \n",
    "        N3[x_t, a_t] += 1\n",
    "        if x_t == GOAL:\n",
    "            x_t = reset()\n",
    "        else:\n",
    "            x_t = x_t1\n",
    "\n",
    "simulate()\n",
    "print(Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Temporal-difference learning\n",
    "\n",
    "You will now run both Q-learning and SARSA, and compare their learning performance with that of the model-based method just studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 4.        \n",
    "\n",
    "Repeat Activity 3 but using the $Q$-learning algorithm with a learning rate $\\alpha=0.3$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 5.\n",
    "\n",
    "Repeat Activity 4 but using the SARSA algorithm.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Activity 6.\n",
    "\n",
    "Discuss the differences observed between the performance of the three methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your comments here."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
